% --------------------------------------------------- 
%   Introduction
% ---------------------------------------------------

@misc{models_require_alotf_of_computations,
    Author = {Jaime Sevilla and Lennart Heim and Anson Ho and Tamay Besiroglu and Marius Hobbhahn and Pablo Villalobos},
    Title = {Compute Trends Across Three Eras of Machine Learning},
    Year = {2022},
    Eprint = {arXiv:2202.05924},
}


@misc{darpa_ml_application,
  title={Radio Frequency Machine Learning Systems ({RFMLS}) },
  author={Mr. John Davies},
  howpublished={\url{https://www.darpa.mil/program/radio-frequency-machine-learning-systems}},
} 


@article{edge_computing,
    author = {Cao, Keyan and Liu, Yefan and Meng, Gongjie and Sun, Qimeng},
    year = {2020},
    month = {01},
    pages = {1-1},
    title = {An Overview on Edge Computing Research},
    volume = {PP},
    journal = {IEEE Access},
    doi = {10.1109/ACCESS.2020.2991734}
}

@Article{ml_applications,
    author={Sarker, Iqbal H.},
    title={Machine Learning: Algorithms, Real-World Applications and Research Directions},
    journal={SN Computer Science},
    year={2021},
    month={3},
    day={22},
    volume={2},
    number={3},
    pages={160},
    abstract={In the current age of the Fourth Industrial Revolution (4IR or Industry 4.0), the digital world has a wealth of data, such as Internet of Things (IoT) data, cybersecurity data, mobile data, business data, social media data, health data, etc. To intelligently analyze these data and develop the corresponding smart and automated applications, the knowledge of artificial intelligence (AI), particularly, machine learning (ML) is the key. Various types of machine learning algorithms such as supervised, unsupervised, semi-supervised, and reinforcement learning exist in the area. Besides, the deep learning, which is part of a broader family of machine learning methods, can intelligently analyze the data on a large scale. In this paper, we present a comprehensive view on these machine learning algorithms that can be applied to enhance the intelligence and the capabilities of an application. Thus, this study's key contribution is explaining the principles of different machine learning techniques and their applicability in various real-world application domains, such as cybersecurity systems, smart cities, healthcare, e-commerce, agriculture, and many more. We also highlight the challenges and potential research directions based on our study. Overall, this paper aims to serve as a reference point for both academia and industry professionals as well as for decision-makers in various real-world situations and application areas, particularly from the technical point of view.},
    issn={2661-8907},
    doi={10.1007/s42979-021-00592-x},
    url={https://doi.org/10.1007/s42979-021-00592-x}
}

@misc{tflm_and_pytorch_mobile,
    Author = {Chunjie Luo and Xiwen He and Jianfeng Zhan and Lei Wang and Wanling Gao and Jiahui Dai},
    Title = {Comparison and Benchmarking of AI Models and Frameworks on Mobile Devices},
    Year = {2020},
    Eprint = {arXiv:2005.05085},
}

@misc{vexriscv,
  author = {Papon, Charles},
  title = {VexRiscv},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/SpinalHDL/VexRiscv}},
}



@techreport{risc_v_manual,
    Author = {Waterman, Andrew and Lee, Yunsup and Patterson, David A. and Asanović, Krste},
    Title = {The RISC-V Instruction Set Manual, Volume I: User-Level ISA, Version 2.0},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2014},
    Month = {5},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-54.html},
    Number = {UCB/EECS-2014-54}
}
% ---------------------------------------------------
%   Background
% ---------------------------------------------------
@Manual{pysdr,
    title = {PySDR: A Guide to SDR and DSP using Python},
    author = {{Dr. Marc Lichtman}},
    % organization = {R Foundation for Statistical Computing},
    % address = {Vienna, Austria},
    year = 2023,
    % note = {{ISBN} 3-900051-07-0},
    url = {https://pysdr.org/index.html}
}


@Article{amr_deep_networks_overview,
    AUTHOR = {Xiao, Wenshi and Luo, Zhongqiang and Hu, Qian},
    TITLE = {A Review of Research on Signal Modulation Recognition Based on Deep Learning},
    JOURNAL = {Electronics},
    VOLUME = {11},
    YEAR = {2022},
    NUMBER = {17},
    ARTICLE-NUMBER = {2764},
    URL = {https://www.mdpi.com/2079-9292/11/17/2764},
    ISSN = {2079-9292},
    ABSTRACT = {Since the emergence of 5G technology, the wireless communication system has had a huge data throughput, so the joint development of artificial intelligence technology and wireless communication technology is one of the current mainstream development directions. In particular the combination of deep learning technology and communication physical layer technology is the future research hotspot. The purpose of this research paper is to summarize the related algorithms of the combination of Automatic Modulation Recognition (AMR) technology and deep learning technology in the communication physical layer. In order to elicit the advantages of the modulation recognition algorithm based on deep learning, this paper firstly introduces the traditional AMR method, and then summarizes the advantages and disadvantages of the traditional algorithm. Then, the application of the deep learning algorithm in AMR is described, and the identification method based on a typical deep learning network is emphatically described. Afterwards, the existing Deep Learning (DL) modulation identification algorithm in a small sample environment is summarized. Finally, DL modulation is discussed, identifying field challenges, and future research directions.},
    DOI = {10.3390/electronics11172764}
}

@ARTICLE{amr_survey_deep_learning,
  author={Zhang, Chaoyun and Patras, Paul and Haddadi, Hamed},
  journal={IEEE Communications Surveys and Tutorials}, 
  title={Deep Learning in Mobile and Wireless Networking: A Survey}, 
  year={2019},
  volume={21},
  number={3},
  pages={2224-2287},
  doi={10.1109/COMST.2019.2904897}
  
}


@Article{transformer_nlp_applications,
    AUTHOR = {Patwardhan, Narendra and Marrone, Stefano and Sansone, Carlo},
    TITLE = {Transformers in the Real World: A Survey on NLP Applications},
    JOURNAL = {Information},
    VOLUME = {14},
    YEAR = {2023},
    NUMBER = {4},
    ARTICLE-NUMBER = {242},
    URL = {https://www.mdpi.com/2078-2489/14/4/242},
    ISSN = {2078-2489},
    ABSTRACT = {The field of Natural Language Processing (NLP) has undergone a significant transformation with the introduction of Transformers. From the first introduction of this technology in 2017, the use of transformers has become widespread and has had a profound impact on the field of NLP. In this survey, we review the open-access and real-world applications of transformers in NLP, specifically focusing on those where text is the primary modality. Our goal is to provide a comprehensive overview of the current state-of-the-art in the use of transformers in NLP, highlight their strengths and limitations, and identify future directions for research. In this way, we aim to provide valuable insights for both researchers and practitioners in the field of NLP. In addition, we provide a detailed analysis of the various challenges faced in the implementation of transformers in real-world applications, including computational efficiency, interpretability, and ethical considerations. Moreover, we highlight the impact of transformers on the NLP community, including their influence on research and the development of new NLP models.},
    DOI = {10.3390/info14040242}
}




@misc{learning_constellation_map,
    Author = {Van-Sang Doan and Thien Huynh-The and Cam-Hao Hua and Quoc-Viet Pham and Dong-Seong Kim},
    Title = {Learning Constellation Map with Deep CNN for Accurate Modulation Recognition},
    Year = {2020},
    Eprint = {arXiv:2009.02026},
}

% @Manual{,
%     title = {R: A Language and Environment for Statistical Computing},
%     author = {{R Development Core Team}},
%     organization = {R Foundation for Statistical Computing},
%     address = {Vienna, Austria},
%     year = 2011,
%     note = {{ISBN} 3-900051-07-0},
%     url = {http://www.R-project.org}
% } 

@manual{vivado_synthesis,
    title={Vivado Design Suite User Guide: High-Level Synthesis},
    organization = {Xilinx},
    address = {San Jose, 2100 Logic Dr, United States},
    year={2012},
    url={https://docs.xilinx.com/v/u/en-US/ug902-vivado-high-level-synthesis},
}

@manual{vivado_implementation,
    title={Vivado Design Suite User Guide: Implementation},
    organization = {Xilinx},
    address = {San Jose, 2100 Logic Dr, United States},
    year={2012},
    url={https://www.xilinx.com/content/dam/xilinx/support/documents/sw_manuals/xilinx2022_2/ug904-vivado-implementation.pdf},
}

@manual{tf_lite_quantization,
    title={Post-training quantization},
    organization = {Google},
    address = {1600 Amphitheater Parkway Mountain View, CA 94043 USA},
    year={2018},
    url={https://www.tensorflow.org/lite/performance/post_training_quantization},
}

@misc{yosys,
  author = {},
  title = {Yosys Open SYnthesis Suite},
  year = {2013},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/YosysHQ/yosys}},
}

@misc{risc_v_cfu,
    title={Draft Proposed RISC-V Composable Custom Extensions Specification},
    author={Ansell, Tim and Tim, Callahan and Jan, Gray},
    year={2022},
    url={https://github.com/grayresearch/CFU/blob/main/spec/spec.pdf},
}


@inproceedings{LDA_for_simc,
  title={Classification of Digital Modulated Signals Using Linear Discriminant Analysis on Faded Channel},
  author={Sajjad Ahmed Ghauri and Ijaz Mansoor Qureshi and Muhammad Adnan Aziz and Tanveer Ahmed Cheema},
  year={2014}
}

@article{QDA_for_simc,
    title={Application of Discriminant Analysis for Signals Identification in Communication Systems},
    author={Ivelina Balabanova1, Stela Kostadinova2, Boyan Karapenev, Georgi Georgiev},
    year={2020},
    month={02},
    journal = {JOURNAL OF Engineering Science and Technology Review}
}

@INPROCEEDINGS{ML_for_simc,
  author={Sills, J.A.},
  booktitle={MILCOM 1999. IEEE Military Communications. Conference Proceedings (Cat. No.99CH36341)}, 
  title={Maximum-likelihood modulation classification for PSK/QAM}, 
  year={1999},
  volume={1},
  number={},
  pages={217-220 vol.1},
  doi={10.1109/MILCOM.1999.822675}
}

@ARTICLE{svd_af_for_simc,
  author={Guo, Qiang and Nan, Pulong and Zhang, Xiaoyu and Zhao, Yuning and Wan, Jian},
  journal={Journal of Communications and Networks}, 
  title={Recognition of radar emitter signals based on SVD and AF main ridge slice}, 
  year={2015},
  volume={17},
  number={5},
  pages={491-498},
  doi={10.1109/JCN.2015.000087}}

@InProceedings{SVM_for_simc,
    author="Zhao, Zhijin
    and Zhou, Yunshui
    and Mei, Fei
    and Li, Jiandong",
    editor="Yin, Fu-Liang
    and Wang, Jun
    and Guo, Chengan",
    title="Automatic Modulation Classification by Support Vector Machines",
    booktitle="Advances in Neural Networks -- ISNN 2004",
    year="2004",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="654--659",
    abstract="Automatic classification of analog and digital modulation signals plays an important role in communication applications such as an intelligent demodulator, interference identification and monitoring, so many investigations have been carried out in the past. Support Vector Machines (SVMs) maps inputs vectors nonlinearly into a high dimensional feature space and constructs the optimum separating hyperplane in space to realize signal classification. In this paper, a new method based on SVM for classifying AM, FM, BFSK, BPSK, USB and LSB is proposed. The classification results for real communication signals using SVMs are given. Compared with radial basis function neural network (RBFNN) method, the method can classify these signals well, and the correct classification rates are above 82{\%}.",
    isbn="978-3-540-28647-9"
}

@Article{cnn_overview,
    author={Alzubaidi, Laith
    and Zhang, Jinglan
    and Humaidi, Amjad J.
    and Al-Dujaili, Ayad
    and Duan, Ye
    and Al-Shamma, Omran
    and Santamar{\'i}a, J.
    and Fadhel, Mohammed A.
    and Al-Amidie, Muthana
    and Farhan, Laith},
    title={Review of deep learning: concepts, CNN architectures, challenges, applications, future directions},
    journal={Journal of Big Data},
    year={2021},
    month={3},
    day={31},
    volume={8},
    number={1},
    pages={53},
    abstract={In the last few years, the deep learning (DL) computing paradigm has been deemed the Gold Standard in the machine learning (ML) community. Moreover, it has gradually become the most widely used computational approach in the field of ML, thus achieving outstanding results on several complex cognitive tasks, matching or even beating those provided by human performance. One of the benefits of DL is the ability to learn massive amounts of data. The DL field has grown fast in the last few years and it has been extensively used to successfully address a wide range of traditional applications. More importantly, DL has outperformed well-known ML techniques in many domains, e.g., cybersecurity, natural language processing, bioinformatics, robotics and control, and medical information processing, among many others. Despite it has been contributed several works reviewing the State-of-the-Art on DL, all of them only tackled one aspect of the DL, which leads to an overall lack of knowledge about it. Therefore, in this contribution, we propose using a more holistic approach in order to provide a more suitable starting point from which to develop a full understanding of DL. Specifically, this review attempts to provide a more comprehensive survey of the most important aspects of DL and including those enhancements recently added to the field. In particular, this paper outlines the importance of DL, presents the types of DL techniques and networks. It then presents convolutional neural networks (CNNs) which the most utilized DL network type and describes the development of CNNs architectures together with their main features, e.g., starting with the AlexNet network and closing with the High-Resolution network (HR.Net). Finally, we further present the challenges and suggested solutions to help researchers understand the existing research gaps. It is followed by a list of the major DL applications. Computational tools including FPGA, GPU, and CPU are summarized along with a description of their influence on DL. The paper ends with the evolution matrix, benchmark datasets, and summary and conclusion.},
    issn={2196-1115},
    doi={10.1186/s40537-021-00444-8},
    url={https://doi.org/10.1186/s40537-021-00444-8}
}

@misc{vgg16,
    Author = {Karen Simonyan and Andrew Zisserman},
    Title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
    Year = {2014},
    Eprint = {arXiv:1409.1556},
}

@inproceedings{alexnet,
     author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
     pages = {},
     publisher = {Curran Associates, Inc.},
     title = {ImageNet Classification with Deep Convolutional Neural Networks},
     url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
     volume = {25},
     year = {2012}
}


@inproceedings{
    vgg_simc,
    title="Automatic Signal Modulation Recognition based on Deep Convolutional Neural Network",
    author="Yulin, Sun and Jun, Li and Fei, Lin and Guangliang, Pan",
    year="2019" 
}

@misc{cnn_dnn_simc,
    Author = {Timothy J O'Shea and Johnathan Corgan and T. Charles Clancy},
    Title = {Convolutional Radio Modulation Recognition Networks},
    Year = {2016},
    Eprint = {arXiv:1602.04105},
}

@misc{cnn_resnet_cldnn,
    Author = {Xiaoyu Liu and Diyu Yang and Aly El Gamal},
    Title = {Deep Neural Network Architectures for Modulation Classification},
    Year = {2017},
    Eprint = {arXiv:1712.00443},
}

@misc{resnet,
    Author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    Title = {Deep Residual Learning for Image Recognition},
    Year = {2015},
    Eprint = {arXiv:1512.03385},
}

@misc{image_transformer,
    Author = {Niki Parmar and Ashish Vaswani and Jakob Uszkoreit and Łukasz Kaiser and Noam Shazeer and Alexander Ku and Dustin Tran},
    Title = {Image Transformer},
    Year = {2018},
    Eprint = {arXiv:1802.05751},
}

@misc{densenet,
    Author = {Gao Huang and Zhuang Liu and Laurens van der Maaten and Kilian Q. Weinberger},
    Title = {Densely Connected Convolutional Networks},
    Year = {2016},
    Eprint = {arXiv:1608.06993},
}

@INPROCEEDINGS{cldnn,
  author={Sainath, Tara N. and Vinyals, Oriol and Senior, Andrew and Sak, Haşim},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Convolutional, Long Short-Term Memory, fully connected Deep Neural Networks}, 
  year={2015},
  volume={},
  number={},
  pages={4580-4584},
  doi={10.1109/ICASSP.2015.7178838}
}

@article{lstm,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    year = {1997},
    month = {12},
    pages = {1735-80},
    title = {Long Short-term Memory},
    volume = {9},
    journal = {Neural computation},
    doi = {10.1162/neco.1997.9.8.1735}
}


@misc{object_detection_transformer,
    Author = {Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},
    Title = {End-to-End Object Detection with Transformers},
    Year = {2020},
    Eprint = {arXiv:2005.12872},
}

@misc{image_recognition_transformer,
    Author = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    Title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    Year = {2020},
    Eprint = {arXiv:2010.11929},
}

@article{glu_transormer,
    title={TMRN-GLU: A Transformer-Based Automatic Classification Recognition Network Improved by Gate Linear Unit},
    author={Zheng, Yujun and Ma, Yongtao and Tian, Chenglong},
    journal={Electronics},
    volume={11},
    number={10},
    pages={1554},
    year={2022},
    publisher={MDPI}
}

@article{transformer_amr,
    doi = {10.1088/1742-6596/2384/1/012017},
    url = {https://dx.doi.org/10.1088/1742-6596/2384/1/012017},
    year = {2022},
    month = {12},
    publisher = {IOP Publishing},
    volume = {2384},
    number = {1},
    pages = {012017},
    author = {Lingyun Li and Chanchan Qin and Guoqing Li and Shengbo Hu and Yike Xie and Zhenwei Lei},
    title = {Transformer-based radio modulation mode recognition},
    journal = {Journal of Physics: Conference Series},
}

@book{communication_systems_engineering, 
    author={Pearson, Education and Masoud, Salehi},
    title={Communication systems engineering},
    year={2008},
    publisher={Tom Robbins},
    edition={2nd},
    pages={70-143}
}

@article{analog_digital_overview,
  author  = "Amritpal, Kaur",
  title   = "Analog \& Digital Modulation Techniques: An Overview",
  journal = "IJESRT",
  year    = 2014,
}

@book{fundamentals_or_wireless,
  title     = "Fundamentals of Wireless Communication",
  author    = "David, Tse and Pramod, Viswanath",
  year      = 2004,
  publisher = "Cambridge University Press",
}

Proakis, J. G., & Salehi, M. (2008). Communication systems engineering. Pearson Education.

@article{cognitive_radio_amr,
    author = {CHEN, Mei and ZHU, Qi},
    year = {2010},
    month = {04},
    pages = {46-71},
    title = {Cooperative automatic modulation recognition in cognitive radio},
    volume = {2010},
    journal = {The Journal of China Universities of Posts and Telecommunications},
    doi = {10.1016/S1005-8885(09)60445-3}
}

@INPROCEEDINGS{amr_warfare,
  author={Akyön, Fatih Çağatay and Alp, Yaşar Kemal and Gök, Gökhan and Arıkan, Orhan},
  booktitle={2018 26th Signal Processing and Communications Applications Conference (SIU)}, 
  title={Deep learning in electronic warfare systems: Automatic intra-pulse modulation recognition}, 
  year={2018},
  volume={},
  number={},
  pages={1-4},
  doi={10.1109/SIU.2018.8404294}
}

@article{gpgpu_for_dl,
    title = {A survey of techniques for optimizing deep learning on GPUs},
    journal = {Journal of Systems Architecture},
    volume = {99},
    pages = {101635},
    year = {2019},
    issn = {1383-7621},
    doi = {https://doi.org/10.1016/j.sysarc.2019.101635},
    url = {https://www.sciencedirect.com/science/article/pii/S1383762119302656},
    author = {Sparsh Mittal and Shraiysh Vaishay},
    keywords = {Review, GPU, Hardware architecture for deep learning, Accelerator, Distributed training, Parameter server, Allreduce, Pruning, Tiling},
    abstract = {The rise of deep-learning (DL) has been fuelled by the improvements in accelerators. Due to its unique features, the GPU continues to remain the most widely used accelerator for DL applications. In this paper, we present a survey of architecture and system-level techniques for optimizing DL applications on GPUs. We review techniques for both inference and training and for both single GPU and distributed system with multiple GPUs. We bring out the similarities and differences of different works and highlight their key attributes. This survey will be useful for both novice and experts in the field of machine learning, processor architecture and high-performance computing.}
}

@misc{tpu,
    Author = {Norman P. Jouppi and Cliff Young and Nishant Patil and David Patterson and Gaurav Agrawal and Raminder Bajwa and Sarah Bates and Suresh Bhatia and Nan Boden and Al Borchers and Rick Boyle and Pierre-luc Cantin and Clifford Chao and Chris Clark and Jeremy Coriell and Mike Daley and Matt Dau and Jeffrey Dean and Ben Gelb and Tara Vazir Ghaemmaghami and Rajendra Gottipati and William Gulland and Robert Hagmann and C. Richard Ho and Doug Hogberg and John Hu and Robert Hundt and Dan Hurt and Julian Ibarz and Aaron Jaffey and Alek Jaworski and Alexander Kaplan and Harshit Khaitan and Andy Koch and Naveen Kumar and Steve Lacy and James Laudon and James Law and Diemthu Le and Chris Leary and Zhuyuan Liu and Kyle Lucke and Alan Lundin and Gordon MacKean and Adriana Maggiore and Maire Mahony and Kieran Miller and Rahul Nagarajan and Ravi Narayanaswami and Ray Ni and Kathy Nix and Thomas Norrie and Mark Omernick and Narayana Penukonda and Andy Phelps and Jonathan Ross and Matt Ross and Amir Salek and Emad Samadiani and Chris Severn and Gregory Sizikov and Matthew Snelham and Jed Souter and Dan Steinberg and Andy Swing and Mercedes Tan and Gregory Thorson and Bo Tian and Horia Toma and Erick Tuttle and Vijay Vasudevan and Richard Walter and Walter Wang and Eric Wilcox and Doe Hyun Yoon},
    Title = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
    Year = {2017},
    Eprint = {arXiv:1704.04760},
}

@inproceedings{reno,
    author = {Liu, Xiaoxiao and Mao, Mengjie and Liu, Beiye and Li, Hai and Chen, Yiran and Li, Boxun and Wang, Yu and Jiang, Hao and Barnell, Mark and Wu, Qing and Yang, Jianhua},
    title = {RENO: A High-Efficient Reconfigurable Neuromorphic Computing Accelerator Design},
    year = {2015},
    isbn = {9781450335201},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2744769.2744900},
    doi = {10.1145/2744769.2744900},
    abstract = {Neuromorphic computing is recently gaining significant attention as a promising candidate to conquer the well-known von Neumann bottleneck. In this work, we propose RENO -- a efficient reconfigurable neuromorphic computing accelerator. RENO leverages the extremely efficient mixed-signal computation capability of memristor-based crossbar (MBC) arrays to speedup the executions of artificial neural networks (ANNs). The hierarchically arranged MBC arrays can be configured to a variety of ANN topologies through a mixed-signal interconnection network (M-Net). Simulation results on seven ANN applications show that compared to the baseline general-purpose processor, RENO can achieve on average 178.4x (27.06x) performance speedup and 184.2x (25.23x) energy savings in high-efficient multilayer perception (high-accurate auto-associative memory) implementation. Moreover, in the comparison to a pure digital neural processing unit (D-NPU) and a design with MBC arrays co-operating through a digital interconnection network, RENO still achieves the fastest execution time and the lowest energy consumption with similar computation accuracy.},
    booktitle = {Proceedings of the 52nd Annual Design Automation Conference},
    articleno = {66},
    numpages = {6},
    location = {San Francisco, California},
    series = {DAC '15}
}

@article{accelerators,
    title = {A Survey of Accelerator Architectures for Deep Neural Networks},
    journal = {Engineering},
    volume = {6},
    number = {3},
    pages = {264-274},
    year = {2020},
    issn = {2095-8099},
    doi = {https://doi.org/10.1016/j.eng.2020.01.007},
    url = {https://www.sciencedirect.com/science/article/pii/S2095809919306356},
    author = {Yiran Chen and Yuan Xie and Linghao Song and Fan Chen and Tianqi Tang},
    keywords = {Deep neural network, Domain-specific architecture, Accelerator},
    abstract = {Recently, due to the availability of big data and the rapid growth of computing power, artificial intelligence (AI) has regained tremendous attention and investment. Machine learning (ML) approaches have been successfully applied to solve many problems in academia and in industry. Although the explosion of big data applications is driving the development of ML, it also imposes severe challenges of data processing speed and scalability on conventional computer systems. Computing platforms that are dedicatedly designed for AI applications have been considered, ranging from a complement to von Neumann platforms to a “must-have” and stand-alone technical solution. These platforms, which belong to a larger category named “domain-specific computing,” focus on specific customization for AI. In this article, we focus on summarizing the recent advances in accelerator designs for deep neural networks (DNNs)—that is, DNN accelerators. We discuss various architectures that support DNN executions in terms of computing units, dataflow optimization, targeted network topologies, architectures on emerging technologies, and accelerators for emerging applications. We also provide our visions on the future trend of AI chip designs.}
}

  

@misc{hls4ml,
    Author = {Farah Fahim and Benjamin Hawks and Christian Herwig and James Hirschauer and Sergo Jindariani and Nhan Tran and Luca P. Carloni and Giuseppe Di Guglielmo and Philip Harris and Jeffrey Krupa and Dylan Rankin and Manuel Blanco Valentin and Josiah Hester and Yingyi Luo and John Mamish and Seda Orgrenci-Memik and Thea Aarrestad and Hamza Javed and Vladimir Loncar and Maurizio Pierini and Adrian Alan Pol and Sioni Summers and Javier Duarte and Scott Hauck and Shih-Chieh Hsu and Jennifer Ngadiuba and Mia Liu and Duc Hoang and Edward Kreinar and Zhenbin Wu},
    Title = {hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices},
    Year = {2021},
    Eprint = {arXiv:2103.05579},
}

@INPROCEEDINGS{npu,
  author={Esmaeilzadeh, Hadi and Sampson, Adrian and Ceze, Luis and Burger, Doug},
  booktitle={2012 45th Annual IEEE/ACM International Symposium on Microarchitecture}, 
  title={Neural Acceleration for General-Purpose Approximate Programs}, 
  year={2012},
  volume={},
  number={},
  pages={449-460},
  doi={10.1109/MICRO.2012.48}}

@misc{renode,
  author = {},
  title = {Renode},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/renode/renode}},
}

% @INPROCEEDINGS{fpga_overview,
%   author={Qasim, Syed Manzoor and Abbasi, Shuja Ahmad and Almashary, Bandar},
%   booktitle={2009 International Multimedia, Signal Processing and Communication Technologies}, 
%   title={An overview of advanced FPGA architectures for optimized hardware realization of computation intensive algorithms}, 
%   year={2009},
%   volume={},
%   number={},
%   pages={300-303},
%   doi={10.1109/MSPCT.2009.5164235}
% }


@ARTICLE{fpga_architecture,
  author={Boutros, Andrew and Betz, Vaughn},
  journal={IEEE Circuits and Systems Magazine}, 
  title={FPGA Architecture: Principles and Progression}, 
  year={2021},
  volume={21},
  number={2},
  pages={4-29},
  doi={10.1109/MCAS.2021.3071607}
}



@ARTICLE{fpga_vs_asic,
  author={Kuon, Ian and Rose, Jonathan},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems}, 
  title={Measuring the Gap Between FPGAs and ASICs}, 
  year={2007},
  volume={26},
  number={2},
  pages={203-215},
  doi={10.1109/TCAD.2006.884574}
}


@Article{fpga_high_throughput,
    author={Vanderbauwhede, Wim
    and Chalamalasetti, S. R.
    and Margala, M.},
    title={Throughput Analysis for a High-Performance FPGA-Accelerated Real-Time Search Application},
    journal={International Journal of Reconfigurable Computing},
    year={2012},
    month={2},
    day={15},
    publisher={Hindawi Publishing Corporation},
    volume={2012},
    pages={507173},
    abstract={We propose an FPGA design for the relevancy computation part of a high-throughput real-time search application. The application matches terms in a stream of documents against a static profile, held in off-chip memory. We present a mathematical analysis of the throughput of the application and apply it to the problem of scaling the Bloom filter used to discard nonmatches.},
    issn={1687-7195},
    doi={10.1155/2012/507173},
    url={https://doi.org/10.1155/2012/507173}
}

@article{fpga_to_asic,
    author = {Fang, WenHai and Spaanenburg, Lambert},
    year = {2007},
    month = {06},
    pages = {},
    title = {Power-driven FPGA to ASIC conversion - art. no. 659005},
    journal = {Proceedings of SPIE - The International Society for Optical Engineering},
    doi = {10.1117/12.722901}
}




@misc{cpu_vs_gpu_vs_fpga,
    Author = {Murad Qasaimeh and Kristof Denolf and Jack Lo and Kees Vissers and Joseph Zambreno and Phillip H. Jones},
    Title = {Comparing Energy Efficiency of CPU, GPU and FPGA Implementations for Vision Kernels},
    Year = {2019},
    Eprint = {arXiv:1906.11879},
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@incollection{pytorch,
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
    booktitle = {Advances in Neural Information Processing Systems 32},
    pages = {8024--8035},
    year = {2019},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
} 

@misc{fpga_diagram,
    title={What are {FPGAs}?},
    author={Christos, Kyrkou},
    year={2017},
    url={https://ckyrkou.medium.com/what-are-fpgas-c9121ac2a7ae},
}

% ----------------------------
%  Methodology
% ----------------------------

@misc{cfu_playground,
    Author = {Shvetank Prakash and Tim Callahan and Joseph Bushagour and Colby Banbury and Alan V. Green and Pete Warden and Tim Ansell and Vijay Janapa Reddi},
    Title = {CFU Playground: Full-Stack Open-Source Framework for Tiny Machine Learning (tinyML) Acceleration on FPGAs},
    Year = {2022},
    Eprint = {arXiv:2201.01863},
}

@misc{tflm,
    Author = {Robert David and Jared Duke and Advait Jain and Vijay Janapa Reddi and Nat Jeffries and Jian Li and Nick Kreeger and Ian Nappier and Meghna Natraj and Shlomi Regev and Rocky Rhodes and Tiezhen Wang and Pete Warden},
    Title = {TensorFlow Lite Micro: Embedded Machine Learning on TinyML Systems},
    Year = {2020},
    Eprint = {arXiv:2010.08678},
}

@ARTICLE{verilog_standard, 
  author={},
  journal={IEEE Std 1364-2005 (Revision of IEEE Std 1364-2001)}, 
  title={IEEE Standard for Verilog Hardware Description Language}, 
  year={2006},
  volume={},
  number={},
  pages={1-590},
  doi={10.1109/IEEESTD.2006.99495}
}

@ARTICLE{system_verilog_standard,
  author={},
  journal={IEEE Std 1800-2017 (Revision of IEEE Std 1800-2012)}, 
  title={IEEE Standard for SystemVerilog--Unified Hardware Design, Specification, and Verification Language}, 
  year={2018},
  volume={},
  number={},
  pages={1-1315},
  doi={10.1109/IEEESTD.2018.8299595}
}

@article{vtr8,
  title={VTR 8: High Performance CAD and Customizable FPGA Architecture Modelling},
  author={Murray, Kevin E. and Petelin, Oleg and Zhong, Sheng and Wang, Jai Min and ElDafrawy, Mohamed and Legault, Jean-Philippe and Sha, Eugene and Graham, Aaron G. and Wu, Jean and Walker, Matthew J. P. and Zeng, Hanqing and Patros, Panagiotis and Luu, Jason and Kent, Kenneth B. and Betz, Vaughn},
  journal={ACM Trans. Reconfigurable Technol. Syst.},
  year={2020}
}

@misc{f4pga,
  author = {},
  title = {FOSS Flows For FPGA},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/chipsalliance/f4pga}},
}




% 
%   Classification models
% 

@ARTICLE{eye_diagram_simc,
  author={Wang, Danshi and Zhang, Min and Li, Ze and Li, Jin and Fu, Meixia and Cui, Yue and Chen, Xue},
  journal={IEEE Photonics Technology Letters}, 
  title={Modulation Format Recognition and OSNR Estimation Using CNN-Based Deep Learning}, 
  year={2017},
  volume={29},
  number={19},
  pages={1667-1670},
  doi={10.1109/LPT.2017.2742553}}


@ARTICLE{radioml_2018,
  author={O’Shea, Timothy James and Roy, Tamoghna and Clancy, T. Charles},
  journal={IEEE Journal of Selected Topics in Signal Processing}, 
  title={Over-the-Air Deep Learning Based Radio Signal Classification}, 
  year={2018},
  volume={12},
  number={1},
  pages={168-179},
  doi={10.1109/JSTSP.2018.2797022}
}



 @misc{constelation_img,
   author = "Wikimedia Commons",
   title = "File:8PSK Gray Coded.svg --- Wikimedia Commons{,} the free media repository",
   year = "2020",
   url = "https://commons.wikimedia.org/w/index.php?title=File:8PSK_Gray_Coded.svg&oldid=454679020",
   note = "[Online; accessed 14-May-2023]"
 }


@ARTICLE{time_freq_diagram_simc,
  title    = "Radar Signal Modulation Recognition Based on {Sep-ResNet}",
  author   = "Mao, Yongjiang and Ren, Wenjuan and Yang, Zhanpeng",
  abstract = "With the development of signal processing technology and the use
              of new radar systems, signal aliasing and electronic interference
              have occurred in space. The electromagnetic signals have become
              extremely complicated in their current applications in space,
              causing difficult problems in terms of accurately identifying
              radar-modulated signals in low signal-to-noise ratio (SNR)
              environments. To address this problem, in this paper, we propose
              an intelligent recognition method that combines time-frequency
              (T-F) analysis and a deep neural network to identify radar
              modulation signals. The T-F analysis of the complex Morlet
              wavelet transform (CMWT) method is used to extract the
              characteristics of signals and obtain the T-F images. Adaptive
              filtering and morphological processing are used in T-F image
              enhancement to reduce the interference of noise on signal
              characteristics. A deep neural network with the channel-separable
              ResNet (Sep-ResNet) is used to classify enhanced T-F images. The
              proposed method completes high-accuracy intelligent recognition
              of radar-modulated signals in a low-SNR environment. When the SNR
              is -10 dB, the probability of successful recognition (PSR) is
              93.44\%.",
  journal  = "Sensors (Basel)",
  volume   =  21,
  number   =  22,
  month    =  nov,
  year     =  2021,
  address  = "Switzerland",
  keywords = "channel-separable ResNet; complex Morlet wavelet; image
              enhancement; radar modulation signal; time--frequency analysis",
  language = "en"
}

@misc{matlab_model,
  title = {Modulation Classification with Deep Learning},
  howpublished = {\url{https://www.mathworks.com/help/deeplearning/ug/modulation-classification-with-deep-learning.html}},
}

@misc{batch_normalization,
    Author = {Sergey Ioffe and Christian Szegedy},
    Title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
    Year = {2015},
    Eprint = {arXiv:1502.03167},
}

@misc{max_pooling,
    Author = {Hossein Gholamalinezhad and Hossein Khosravi},
    Title = {Pooling Methods in Deep Neural Networks, a Review},
    Year = {2020},
    Eprint = {arXiv:2009.07485},
}

@misc{cnn_intro,
    Author = {Keiron O'Shea and Ryan Nash},
    Title = {An Introduction to Convolutional Neural Networks},
    Year = {2015},
    Eprint = {arXiv:1511.08458},
}

@article{cnn_radio,
    author = {Riyaz, Shamnaz and Sankhe, Kunal and Ioannidis, Stratis and Chowdhury, Kaushik},
    year = {2018},
    month = {09},
    pages = {146-152},
    title = {Deep Learning Convolutional Neural Networks for Radio Identification},
    volume = {56},
    journal = {IEEE Communications Magazine},
    doi = {10.1109/MCOM.2018.1800153}
}

@misc{attention_is_all_you_need,
    Author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    Title = {Attention Is All You Need},
    Year = {2017},
    Eprint = {arXiv:1706.03762},
}

@misc{keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
} 

@misc{gemmlowp,
  title = {gemmlowp: a small self-contained low-precision GEMM library},
  year = {2015},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/google/gemmlowp}},
}

% Chapter 5
@misc{amaranth,
  title = {Amaranth HDL},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/amaranth-lang/amaranth}},
}

@misc{quantization_survey,
    Author = {Babak Rokh and Ali Azarpeyvand and Alireza Khanteymoori},
    Title = {A Comprehensive Survey on Model Quantization for Deep Neural Networks},
    Year = {2022},
    Eprint = {arXiv:2205.07877},
}

% 
%   Conclusion
% 

@misc{different_transformers,
    Author = {Sharan Narang and Hyung Won Chung and Yi Tay and William Fedus and Thibault Fevry and Michael Matena and Karishma Malkan and Noah Fiedel and Noam Shazeer and Zhenzhong Lan and Yanqi Zhou and Wei Li and Nan Ding and Jake Marcus and Adam Roberts and Colin Raffel},
    Title = {Do Transformer Modifications Transfer Across Implementations and Applications?},
    Year = {2021},
    Eprint = {arXiv:2102.11972},
}

@misc{depth_wise_convolution_original,
    Author = {Laurent SIfre and Stéphane Mallat},
    Title = {Rigid-Motion Scattering for Texture Classification},
    Year = {2014},
    Eprint = {arXiv:1403.1687},
}

@misc{mobile_net,
    Author = {Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam},
    Title = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
    Year = {2017},
    Eprint = {arXiv:1704.04861},
}

@misc{xception,
    Author = {François Chollet},
    Title = {Xception: Deep Learning with Depthwise Separable Convolutions},
    Year = {2016},
    Eprint = {arXiv:1610.02357},
}

@misc{cnn_pruning,
    Author = {Zi Wang and Chengcheng Li and Xiangyang Wang},
    Title = {Convolutional Neural Network Pruning with Structural Redundancy Reduction},
    Year = {2021},
    Eprint = {arXiv:2104.03438},
}

@INPROCEEDINGS{cnn_fusion,
  author={Lavinia, Yukhe and Vo, Holly H. and Verma, Abhishek},
  booktitle={2016 IEEE International Symposium on Multimedia (ISM)}, 
  title={Fusion Based Deep CNN for Improved Large-Scale Image Action Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={609-614},
  doi={10.1109/ISM.2016.0131}
}

@misc{cnn_pruning2,
    Author = {Hao Li and Asim Kadav and Igor Durdanovic and Hanan Samet and Hans Peter Graf},
    Title = {Pruning Filters for Efficient ConvNets},
    Year = {2016},
    Eprint = {arXiv:1608.08710},
}

@misc{speeding_up_survey,
    Author = {Gaurav Menghani},
    Title = {Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better},
    Year = {2021},
    Eprint = {arXiv:2106.08962},
}

@misc{ofa_network,
    Author = {Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},
    Title = {Once-for-All: Train One Network and Specialize it for Efficient Deployment},
    Year = {2019},
    Eprint = {arXiv:1908.09791},
}

@Manual{matbal_dl_hdl_documentation,
    title = {R: A Language and Environment for Statistical Computing},
    organization = {MathWorks, Inc},
    address = {Three Apple Hill Drive Natick, MA 01760 USA},
    url = {https://www.mathworks.com/help/deep-learning-hdl/}
} 



% Software citation: https://www.software.ac.uk/how-cite-software
% 
% @Manual{,
%     title = {R: A Language and Environment for Statistical Computing},
%     author = {{R Development Core Team}},
%     organization = {R Foundation for Statistical Computing},
%     address = {Vienna, Austria},
%     year = 2011,
%     note = {{ISBN} 3-900051-07-0},
%     url = {http://www.R-project.org}
% } 