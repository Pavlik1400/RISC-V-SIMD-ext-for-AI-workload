%----------------------------------------------------------------------------------------
%	Conclusions
%----------------------------------------------------------------------------------------

\chapter{Conclusions and Future work} \label{Chapter 6}

\section{Conclusions}
In this work, we explore the Automatic Modulation Recognition problem. The problem is researched, and modern classical and deep learning solutions are explored. We implemented a convolutional neural and transformed-based neural network to solve this problem. Models are trained, fine-tuned, and evaluated on two different datasets~\cite{cnn_dnn_simc,matlab_model}. The impact of different characteristics of datasets and model hyper-parameters and quantization on performance is analyzed. After training and evaluating, one configuration of the CNN model was chosen as the target model for acceleration. 

This work gave an overview of existing hardware accelerators for AI applications. One such approach -- Custom Function Unit extension for RISC-V soft-core processor implemented in FPGA was chosen to accelerate Neural Networks for inference on resource-constrained embedded devices. The pros and cons of such an approach are shown, and all parts, such as RISC-V architecture, FPGA, HDLs, Hardware simulation, and synthesis tools, are reviewed. 

CNN was quantized and exported as Tensorflow Lite micro model and integrated into the CFU-Playground framework. After that, we profile the model and find the bottleneck -- convolutional layer. Implementation of a convolutional layer in TFLM is reviewed and changed to integrate with the accelerator. Accelerator was developed iteratively; all iterations are described. The final design is depicted in Fig.~\cref{fig:cfu_v8_functional_diagram,fig:cfu_v8_state_machine}. Experiments were conducted in simulation and hardware with the Xilinx Arty A7-100T development board. Resulting speed up, inference time, FPGA resource utilization, and power consumption are described. 

\section{Future work}
There are many points worth exploring in future work. First of all, it's worth deploying and profiling the encoder model. It is more complex, but there can be more bottlenecks worth accelerating. Also, more experiments on the models should be conducted. We used the ''vanilla'' encoder, proposed in~\cite{attention_is_all_you_need}, but there are many different variations of transformer~\cite{different_transformers} architecture that can potentially work well for this problem. It is also worth trying depth-wise CNN~\cite{depth_wise_convolution_original,mobile_net,xception}, or other optimization methods such as pruning~\cite{cnn_pruning,cnn_pruning2}, fusion~\cite{cnn_fusion}, and other~\cite{ofa_network,speeding_up_survey}.

The current accelerator design can be improved in terms of both resource utilization and fitting timing constraints. The latter is very important since, for experiments, the clock frequency was lowered to 30MHz, while VexRiscV works on 75MHz on board used for experiments, and the board itself supported frequency up to 450MHz. Architectural changes should also be explored. For example, the input buffer is utilized only on the biggest model layer. Theoretically, more computations per cycle can be achieved with pipelining and a more clever utilization of block RAM. Also, it is worth exploring ways to generalize the accelerator, and generalization impact on performance, since the current design makes some assumptions about model configuration. Finally, frameworks that directly synthesize neural networks to the FPGA or SOC, like hls4ml~\cite{hls4ml} and Deep Learning HDL Toolbox~\cite{matbal_dl_hdl_documentation} should be compared to the method used in this work.